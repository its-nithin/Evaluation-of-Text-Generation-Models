ğŸ§ª Evaluation of Text-Generation Models using Gemini & ROUGE

This project demonstrates how to evaluate text-generation models (specifically Googleâ€™s Gemini) on tasks like summarization, translation, and question answering.
It integrates a custom ROUGE-N metric implementation to compare model outputs against reference text.

ğŸš€ Features

ğŸ”— Uses Google Gemini API (gemini-pro)

ğŸ“Š Implements ROUGE-N (recall, precision, F1) from scratch

ğŸ¯ Interactive CLI menu for:

Summarization

Translation

Question Answering

ğŸ§© Works directly in Google Colab or as a .py script

âš™ï¸ Installation

Run this in Google Colab or your local environment:

pip install -q -U google-generativeai

ğŸ”‘ Setup

Get your Google Generative AI API Key: Google AI Studio

Add your API key in the script:

genai.configure(api_key="YOUR_API_KEY")

â–¶ï¸ Usage

Run the script in Colab or locally:

Choose an option:
1. Summarization
2. Translation
3. Question answering
4. Exit

Example Run
Text generated by the reference Model:

Exercise triggers diverse cellular and molecular changes in 19 rat organs...

Rouge Score: 0.63

ğŸ“Š Example Tasks
Task	Description	Example Result
ğŸ“ Summarization	Shorten long passages	ROUGE = 0.63
ğŸŒ Translation	Convert text between languages	ROUGE = 0.71
â“ QA	Answer natural language questions	ROUGE = 0.68
ğŸ“œ License

Licensed under MIT License
