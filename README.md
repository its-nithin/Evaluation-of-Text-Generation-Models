🧪 Evaluation of Text-Generation Models using Gemini & ROUGE

This project demonstrates how to evaluate text-generation models (specifically Google’s Gemini) on tasks like summarization, translation, and question answering.
It integrates a custom ROUGE-N metric implementation to compare model outputs against reference text.

🚀 Features

🔗 Uses Google Gemini API (gemini-pro)

📊 Implements ROUGE-N (recall, precision, F1) from scratch

🎯 Interactive CLI menu for:

Summarization

Translation

Question Answering

🧩 Works directly in Google Colab or as a .py script

⚙️ Installation

Run this in Google Colab or your local environment:

pip install -q -U google-generativeai

🔑 Setup

Get your Google Generative AI API Key: Google AI Studio

Add your API key in the script:

genai.configure(api_key="YOUR_API_KEY")

▶️ Usage

Run the script in Colab or locally:

Choose an option:
1. Summarization
2. Translation
3. Question answering
4. Exit

Example Run
Text generated by the reference Model:

Exercise triggers diverse cellular and molecular changes in 19 rat organs...

Rouge Score: 0.63

📊 Example Tasks
Task	Description	Example Result
📝 Summarization	Shorten long passages	ROUGE = 0.63
🌍 Translation	Convert text between languages	ROUGE = 0.71
❓ QA	Answer natural language questions	ROUGE = 0.68
📜 License

Licensed under MIT License
